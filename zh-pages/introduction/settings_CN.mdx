# LLM设置

在使用提示时，您将通过API或直接与LLM进行交互。您可以配置一些参数以获得不同提示的结果。

**温度** - 简而言之，温度越低，结果就越确定，因为始终选择最可能的下一个标记。增加温度可能会导致更多的随机性，从而鼓励更多样化或创造性的输出。我们实际上是增加了其他可能标记的权重。在应用方面，我们可能希望对于基于事实的问答等任务使用较低的温度值，以鼓励更加事实和简洁的回答。对于生成诗歌或其他创意任务，增加温度值可能会更有益。

**Top_p** - 同样，使用温度的一种采样技术称为核心采样，您可以控制模型在生成响应时的确定性。如果您正在寻找确切和事实的答案，请将其保持较低。如果您正在寻找更多样化的答案，请将其增加到较高的值。

一般建议是只更改其中一个参数。

在开始一些基本示例之前，请记住，您的结果可能会因您使用的LLM版本而异。