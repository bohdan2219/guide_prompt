# 论文

以下是关于提示工程的最新论文（按发布日期排序）。我们每天更新，新论文不断涌现。我们每周将这些论文的摘要整合到上面的指南中。

## 概述

  - [增强语言模型：一项调查](https://arxiv.org/abs/2302.07842)（2023年2月）
  - [上下文学习的调查](https://arxiv.org/abs/2301.00234)（2022年12月）
  - [面向大型语言模型的推理：一项调查](https://arxiv.org/abs/2212.10403)（2022年12月）
  - [使用语言模型提示进行推理：一项调查](https://arxiv.org/abs/2212.09597)（2022年12月）
  - [大型语言模型的新兴能力](https://arxiv.org/abs/2206.07682)（2022年6月）
  - [文本到图像生成的提示修改器分类法](https://arxiv.org/abs/2204.13988)（2022年4月）
  - [自然语言处理中提示方法的系统调查：预训练、提示和预测](https://arxiv.org/abs/2107.13586)（2021年7月）

## 方法

  - [知识引导上下文优化的视觉-语言提示调整](https://arxiv.org/abs/2303.13283)（2023年3月）
  - [大型语言模型的公平性引导少样本提示](https://arxiv.org/abs/2303.13217)（2023年3月）
  - [大型语言模型的上下文保真提示](https://arxiv.org/abs/2303.11315)（2023年3月）
  - [提示是全部吗？指导性学习的全面和更广泛的视角](https://arxiv.org/abs/2303.10475)（2023年3月）
  - [UPRISE：用于改善零样本评估的通用提示检索](https://arxiv.org/abs/2303.08518)（2023年3月）
  - [通过提示微调使NLP模型对抗攻击](https://arxiv.org/abs/2303.07320)（2023年3月）
  - [结构预训练和提示微调用于知识图谱转移](https://arxiv.org/abs/2303.03922)（2023年3月）
  - [CoTEVer：用于解释验证的思维链提示注释工具包](https://arxiv.org/abs/2303.03628)（2023年3月）
  - [大型语言模型以不同的方式进行上下文学习](https://arxiv.org/abs/2303.03846)（2023年3月）
  - [OpenICL：用于上下文学习的开源框架](https://arxiv.org/abs/2303.02913)（2023年3月）
  - [动态提示：提示微调的统一框架](https://arxiv.org/abs/2303.02909)（2023年3月）
  - [多任务提示微调实现参数高效的迁移学习](https://arxiv.org/abs/2303.02861)（2023年3月）
  - [有限数据的前缀微调数据增强的有效性](https://arxiv.org/abs/2303.02577)（2023年3月）
  - [软提示混合用于可控数据生成](https://arxiv.org/abs/2303.01580)（2023年3月）
  - [提示、生成、缓存：基础模型级联使得强少样本学习器](https://arxiv.org/abs/2303.02151)（2023年3月）
  - [GPT-3.5对前身的鲁棒性如何？关于语言理解任务的全面研究](https://arxiv.org/abs/2303.00293)（2023年3月）
  - [Chat## 论文

- [Promptagator: Few-shot Dense Retrieval From 8 Examples](https://arxiv.org/abs/2209.11755) (2022年9月)
- [Atlas: Few-shot Learning with Retrieval Augmented Language Models](https://arxiv.org/abs/2208.03299) (2022年11月)
- [DocPrompting: Generating Code by Retrieving the Docs](https://arxiv.org/abs/2207.05987) (2022年7月)
- [On the Advance of Making Language Models Better Reasoners](https://arxiv.org/abs/2206.02336) (2022年6月)
- [Large Language Models are Zero-Shot Reasoners](https://arxiv.org/abs/2205.11916) (2022年5月)
- [Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations](https://arxiv.org/abs/2205.11822) (2022年5月)
- [MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning](https://arxiv.org/abs/2205.00445) (2022年5月)
- [PPT: Pre-trained Prompt Tuning for Few-shot Learning](https://aclanthology.org/2022.acl-long.576/) (2022年5月)
- [Toxicity Detection with Generative Prompt-based Inference](https://arxiv.org/abs/2205.12390) (2022年5月)
- [Learning to Transfer Prompts for Text Generation](https://arxiv.org/abs/2205.01543) (2022年5月)
- [The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning](https://arxiv.org/abs/2205.03401) (2022年5月)
- [A Taxonomy of Prompt Modifiers for Text-To-Image Generation](https://arxiv.org/abs/2204.13988) (2022年4月)
- [PromptChainer: Chaining Large Language Model Prompts through Visual Programming](https://arxiv.org/abs/2203.06566) (2022年3月)
- [Self-Consistency Improves Chain of Thought Reasoning in Language Models](https://arxiv.org/abs/2203.11171) (2022年3月)
- [Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155) (2022年3月)
- [Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?](https://arxiv.org/abs/2202.12837) (2022年2月)
- [Chain of Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903) (2022年1月)
- [Show Your Work: Scratchpads for Intermediate Computation with Language Models](https://arxiv.org/abs/2112.00114) (2021年11月)
- [AI Chains: Transparent and Controllable Human-AI Interaction by Chaining Large Language Model Prompts](https://arxiv.org/abs/2110.01691) (2021年10月)
- [Generated Knowledge Prompting for Commonsense Reasoning](https://arxiv.org/abs/2110.08387) (2021年10月)
- [Multitask Prompted Training Enables Zero-Shot Task Generalization](https://arxiv.org/abs/2110.08207) (2021年10月)
- [Reframing Instructional Prompts to GPTk's Language](https://arxiv.org/abs/2109.07830) (2021年9月)
- [Design Guidelines for Prompt Engineering Text-to-Image Generative Models](https://arxiv.org/abs/2109.06977) (2021年9月)
- [Making Pre-trained Language Models  - [提示论文](https://github.com/thunlp/PromptPapers#papers)
  - [提示论文](https://github.com/thunlp/PromptPapers#papers)