# Ohjeistuksen avulla hienosäädettyjen kielimallien skaalaaminen

import {Screenshot} from 'components/screenshot'
import FLAN1 from '../../img/flan-1.png'
import FLAN2 from '../../img/flan-2.png'
import FLAN3 from '../../img/flan-3.png'
import FLAN4 from '../../img/flan-4.png'
import FLAN5 from '../../img/flan-5.png'
import FLAN6 from '../../img/flan-6.png'
import FLAN7 from '../../img/flan-7.png'
import FLAN8 from '../../img/flan-8.png'
import FLAN9 from '../../img/flan-9.png'
import FLAN10 from '../../img/flan-10.png'
import FLAN11 from '../../img/flan-11.png'


<Screenshot src={FLAN1} alt="FLAN1" />
Kuvan Lähde: [Scaling Instruction-Finetuned Language Models](https://arxiv.org/abs/2210.11416)

Tämä tutkimus käsittelee [kielimallien hienosäätämistä ohjeistuksen avulla](https://arxiv.org/pdf/2109.01652.pdf) ja sen vaikutuksia suorituskykyyn ja skaalautumiseen useissa erilaisissa malleissa (PaLM, T5), kehoteasetelmissa (nollakehote, vähäinen ohjaus, ajatusketju(CoT)), ja vertailukohteissa (MMLU, TyDiQA). Tätä tutkitaan seuraavien näkökohtien avulla: tehtävien määrän skaalaaminen (1,8K tehtävää), mallin koon skaalaaminen ja ajatusketjun perusteella tapahtuva hienosäätö (käytetty 9 tietojoukkoa).

**Hienosäätöprosessi:**
- 1,800 tehtävää ilmaistiin ohjeina ja niitä käytettiin mallin hienosäätöön
- Käyttö sekä esimerkkien kanssa että ilman niitä, ja ajatusketjun kanssa ja ilman sitä

Hienosäädettävät tehtävät ja jäävätyt tehtävät on esitetty alla:

<Screenshot src={FLAN11} alt="FLAN11" />

## Kyvyt & keskeiset tulokset

- Ohjeiden hienosäätö skaalautuu hyvin tehtävien määrän ja mallin koon kanssa; tämä viittaa siihen, että tehtävien määrän ja mallin koon skaalaamista on tarpeen jatkaa
- Ajatusketju-tietojoukkojen lisääminen hienosäätöön mahdollistaa hyvän suorituskyvyn päättelytehtävissä
- Flan-PaLM:lla on parantuneet monikieliset kyvyt; 14,9 % parannus nollakehotteen TyDiQA:ssa; 8,1 % parannus aritmeettisessa päättelyssä aliedustetuilla kielillä
- Plan-PaLM suoriutuu hyvin myös avoimen päättelyn kysymyksissä, mikä on hyvä indikaattori parantuneelle käytettävyydelle
- Parantaa suorituskykyä vastuullisen tekoälyn (RAI) vertailuarvojen yli
- Flan-T5-ohjeet, jotka on sovitettu malleihin, osoittavat vahvoja vähäisen ohjauksen kykyjä ja ylittävät julkiset viitearvot, kuten T5


**Tulokset, kun skaalataan hienosäätötehtävien määrää ja mallin kokoa:** mallin koon ja hienosäätötehtävien määrän skaalaamisen odotetaan jatkavan suorituskyvyn parantamista, mutta tehtävien määrän kasvattaminen johtaa heikkenevään kasvuun.

<Screenshot src={FLAN2} alt="FLAN2" />
Kuvan Lähde: [Scaling Instruction-Finetuned Language Models](https://arxiv.org/abs/2210.11416)

**Tulokset hienosäätäessä ilman CoT-dataa ja CoT-dataa käyttäen:** Yhteinen hienosäätö ilman CoT-dataa ja CoT-dataa käyttäen parantaa suorituskykyä molemmissa arvioinneissa verrattuna hienosäätöön käyttäen vain toista.

<Screenshot src={FLAN3} alt="FLAN3" />
Kuvan Lähde: [Scaling Instruction-Finetuned Language Models](https://arxiv.org/abs/2210.11416)

Lisäksi itsejohdonmukaisuus yhdistettynä CoT:hen saavuttaa SOTA-tulokset (State of the Art, kehityksen nykytila) useilla vertailukohdilla. CoT + itsejohdonmukaisuus parantaa merkittävästi tuloksia vertailukohtiin, jotka sisältävät matemaattisia ongelmia (esim. MGSM, GSM8K).

<Screenshot src={FLAN4} alt="FLAN4" />
Kuvan Lähde: [Scaling Instruction-Finetuned Language Models](https://arxiv.org/abs/2210.11416)

CoT-hienosäätö mahdollistaa nollakehote päättelyn, joka aktivoidaan fraasilla "ajatellaan vaihe vaiheelta" (dataa suomenkieliselle testille ei ole), BIG-Bench-tehtävissä. Yleisesti ottaen nollakehote CoT Flan-PaLM suoriutuu paremmin kuin nollakehote CoT PaLM ilman hienosäätöä.

<Screenshot src={FLAN6} alt="FLAN6" />
Kuvan Lähde: [Scaling Instruction-Finetuned Language Models](https://arxiv.org/abs/2210.11416)

Alla on joitain esimerkkejä nollakehote CoT:sta PaLM- ja Flan-PaLM-tehtävissä, joita malli ei ole nähnyt aiemmin.

<Screenshot src={FLAN5} alt="FLAN5" />
Kuvan Lähde: [Scaling Instruction-Finetuned Language Models](https://arxiv.org/abs/2210.11416)

Alla on lisää esimerkkejä nollakehotteista. Se osoittaa, miten PaLM-malli kamppailee toistojen kanssa ja ei vastaa ohjeisiin nollakehote asetuksessa, jossa Flan-PaLM pystyy suoriutumaan hyvin. Vähäisen ohjauksen esimerkit voivat lieventää näitä virheitä.

<Screenshot src={FLAN7} alt="FLAN7" />
Kuvan Lähde: [Scaling Instruction-Finetuned Language Models](https://arxiv.org/abs/2210.11416)

Alla on joitain esimerkkejä, jotka osoittavat lisää Flan-PALM-mallin nollakehote kyvykkyyksiä useilla erityyppisillä haastavilla avoimen päättelyn kysymyksillä:

<Screenshot src={FLAN8} alt="FLAN8" />
Kuvan Lähde: [Scaling Instruction-Finetuned Language Models](https://arxiv.org/abs/2210.11416)


<Screenshot src={FLAN9} alt="FLAN9" />
Kuvan Lähde: [Scaling Instruction-Finetuned Language Models](https://arxiv.org/abs/2210.11416)

<Screenshot src={FLAN10} alt="FLAN10" />
Kuvan Lähde: [Scaling Instruction-Finetuned Language Models](https://arxiv.org/abs/2210.11416)

Voit testata [Flan-T5 malleja Hugging Face Hubissa](https://huggingface.co/google/flan-t5-xxl). 